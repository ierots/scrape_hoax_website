{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "import os \n",
    "import json\n",
    "import sys \n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "###### Import selenium dependencies\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up chrome options for the browser\n",
    "\n",
    "In this section, we can add keyboard arguments to the chrome browser as options. The website https://peter.sh/experiments/chromium-command-line-switches/ contains all the possible extensions to the chrome option class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up chrome Options \n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spider Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hoaxSpider():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.URL=\"http://www.warriorelihoax.com\"\n",
    "        self.content = self.get_request()\n",
    "        self.soup = self.get_soup()\n",
    "        self.cases = self.get_names_cases()\n",
    "        \n",
    "    def get_request(self):\n",
    "        content = requests.get(self.URL).content\n",
    "        return content\n",
    "    \n",
    "    def get_soup(self):\n",
    "        soup = BeautifulSoup(self.content,'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    def get_names_cases(self):\n",
    "        all_tables = self.soup.find_all(id = \"cat\")\n",
    "        txt = \"\"\n",
    "        for i in all_tables:\n",
    "            txt+=i.text.strip()\n",
    "        cases = []\n",
    "        for i in txt.split(\"\\n\"):\n",
    "            reg = re.findall(r\"^[^\\(]+\", i)\n",
    "            cases.append(reg)\n",
    "        flat_list = [item for sublist in cases for item in sublist]\n",
    "        newL = []\n",
    "        for i in flat_list:\n",
    "            newL.append(i.replace(u'\\xa0', u'').replace(\" \", \"-\"))\n",
    "        return newL[1:] \n",
    "    \n",
    "    def select_hoax_case(self, dropdown = '//*[@id=\"cat\"]'):\n",
    "        \n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(\"http://www.warriorelihoax.com\")\n",
    "        python_button = self.driver.find_element_by_xpath(dropdown + '/option[5]')\n",
    "        python_button.click()\n",
    "\n",
    "    def scrape_images(self, dropdown = '//*[@id=\"cat\"]'):\n",
    "        \n",
    "        #init a chrome driver. Add headless option!\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(self.URL)\n",
    "        \n",
    "        # option is the number of cases. This variable is embedded into python_button in order to automatically \n",
    "        # change the pages\n",
    "        option = 1\n",
    "        while option:\n",
    "            time.sleep(2)\n",
    "            \n",
    "            try:\n",
    "                python_button = self.driver.find_element_by_xpath(dropdown + '/option[' + str(option) + ']')\n",
    "         \n",
    "                #get the link for every page. This involves to call the get_names_cases. \n",
    "                new_URL = self.URL + \"/category/\" + self.cases[option-1] + \"/\"\n",
    "                python_button.click()\n",
    "                \n",
    "                #scrape for the images!\n",
    "                self.URL = new_URL\n",
    "                self.content = self.get_request()\n",
    "                self.soup = self.get_soup()\n",
    "                \n",
    "                find_images = self.soup.find_all(\"a\", href = True) #{\"id\": re.compile(r'\\d+')}\n",
    "                find_text = self.soup.find_all(\"p\")\n",
    "                \n",
    "                #create the directories\n",
    "                dir_name = self.cases[option-1]\n",
    "                \n",
    "                if not os.path.exists(\"cases/\" + dir_name):\n",
    "                    os.makedirs(\"cases/\" + dir_name)\n",
    "                    \n",
    "                    if not os.path.exists(\"cases/\" + dir_name + \"/Images\"):\n",
    "                        os.makedirs(\"cases/\" + dir_name + \"/Images\")\n",
    "                        \n",
    "                    if not os.path.exists(\"cases/\" + dir_name + \"/Text\"):\n",
    "                        os.makedirs(\"cases/\" + dir_name + \"/Text\")\n",
    "                \n",
    "                #download images and store to directory\n",
    "                counter = 1\n",
    "                for img in tqdm.tqdm(find_images):\n",
    "                    try:\n",
    "                        # retrieve the URLS from the images\n",
    "                        urllib.request.urlretrieve(img[\"href\"], \"cases/\" + dir_name + \"/Images/\" + \"Img_\" + str(counter) + \".jpg\")\n",
    "                        counter += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                #download text and store to directory \n",
    "                s_text = \"\"\n",
    "                for t in find_text:\n",
    "                    tx = t.text.replace(\"\\xa0\", \"\")\n",
    "                    with open(\"cases/\" + dir_name + \"/Text/text.txt\", 'a') as file:\n",
    "                        file.write(tx + \"\\r\\n\")                    \n",
    "            except:\n",
    "                break\n",
    "            \n",
    "            option += 1 \n",
    "            print(\"---------- case done ----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider = hoaxSpider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_names = spider.get_names_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spider.scrape_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
